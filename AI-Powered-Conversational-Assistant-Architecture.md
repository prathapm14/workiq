# Building Intelligent Conversational AI Assistants: A Complete Architecture Guide

*A comprehensive guide to creating voice-enabled AI assistants with visual representations, multi-modal interactions, and celestial-themed UI*

**Author:** Pratham Reddy (@prathapm14)  
**Date:** January 15, 2026  
**Technologies:** TypeScript, React, Google Gemini API, Web Speech API

---

## Table of Contents

1. [Introduction](#introduction)
2. [System Architecture](#system-architecture)
3. [Core Components](#core-components)
4. [State Management & Types](#state-management--types)
5. [Visual Representation System](#visual-representation-system)
6. [Code Implementation](#code-implementation)
7. [Deployment Architecture](#deployment-architecture)
8. [Best Practices & Patterns](#best-practices--patterns)

---

## Introduction

Modern conversational AI assistants need to go beyond simple text exchanges. This article explores building an intelligent, voice-enabled AI assistant with visual representations, real-time state management, and a unique celestial-themed interface. We'll dive deep into the architecture powering projects like **Pluto** - an AI assistant that thinks, speaks, and visualizes data in innovative ways.

### Key Features

- ğŸ¤ **Voice Input & Output** - Real-time speech recognition and synthesis
- ğŸŒŸ **Multi-Modal Interaction** - Text, voice, and visual responses
- ğŸ¨ **Dynamic Visual Representations** - Charts, graphs, mind maps, and more
- ğŸŒ **Celestial Mode System** - Sun, Moon, Earth, and Neutral themes
- ğŸ§  **AI-Powered Intelligence** - Google Gemini integration for natural conversations
- âš¡ **Real-time State Management** - Smooth transitions between listening, thinking, and speaking

---

## System Architecture

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Orb UI    â”‚  â”‚  Chat Panel  â”‚  â”‚  Controls   â”‚                â”‚
â”‚  â”‚  (Celestial â”‚  â”‚  (Messages)  â”‚  â”‚  (Actions)  â”‚                â”‚
â”‚  â”‚   Modes)    â”‚  â”‚              â”‚  â”‚             â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STATE MANAGEMENT LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PlutoState (IDLE | LISTENING | THINKING | SPEAKING | ...)  â”‚   â”‚
â”‚  â”‚  CelestialMode (SUN | MOON | EARTH | NEUTRAL)               â”‚   â”‚
â”‚  â”‚  Message History, Action Plans, Visual Data                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BUSINESS LOGIC LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Speech    â”‚  â”‚   AI Engine  â”‚  â”‚  Visualization Engine   â”‚   â”‚
â”‚  â”‚  Processing â”‚  â”‚   (Gemini)   â”‚  â”‚  (Chart/Graph/MindMap)  â”‚   â”‚
â”‚  â”‚  (Web API)  â”‚  â”‚              â”‚  â”‚                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTERNAL SERVICES LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Google Gemini   â”‚  â”‚  Web Speech API   â”‚  â”‚  Browser APIs   â”‚ â”‚
â”‚  â”‚  (AI Responses)  â”‚  â”‚  (Voice I/O)      â”‚  â”‚  (Storage, etc) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
User Input (Voice/Text)
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Capture  â”‚â”€â”€â”€â”€â”€â”€â–º Speech Recognition API
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  State   â”‚â”€â”€â”€â”€â”€â”€â–º PlutoState.LISTENING
   â”‚  Update  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Send   â”‚â”€â”€â”€â”€â”€â”€â–º Google Gemini API
   â”‚  to AI   â”‚        (with conversation history)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  State   â”‚â”€â”€â”€â”€â”€â”€â–º PlutoState.THINKING
   â”‚  Update  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Process  â”‚â”€â”€â”€â”€â”€â”€â–º Parse response
   â”‚ Response â”‚        Extract visuals
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        Generate action plans
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  State   â”‚â”€â”€â”€â”€â”€â”€â–º PlutoState.SPEAKING
   â”‚  Update  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Render   â”‚â”€â”€â”€â”€â”€â”€â–º Display text + visuals
   â”‚ Output   â”‚        Speak with TTS
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  State   â”‚â”€â”€â”€â”€â”€â”€â–º PlutoState.IDLE
   â”‚  Update  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Type System Architecture

The foundation of the system is built on strongly-typed TypeScript interfaces that define states, modes, and data structures.

```typescript
// types.ts
export enum PlutoState {
  IDLE = 'IDLE',
  LISTENING = 'LISTENING',
  THINKING = 'THINKING',
  SPEAKING = 'SPEAKING',
  VISUALIZING = 'VISUALIZING',
  ERROR = 'ERROR'
}

export enum CelestialMode {
  SUN = 'SUN',
  MOON = 'MOON',
  EARTH = 'EARTH',
  NEUTRAL = 'NEUTRAL'
}

export interface ActionPlan {
  id: string;
  type: 'open_url' | 'search' | 'info' | 'unknown';
  description: string;
  value?: string;
  confirmed: boolean;
}

export interface Message {
  role: 'user' | 'assistant';
  text: string;
  timestamp: number;
  visual?: VisualData;
}

export type VisualType = 
  | 'MIND_MAP' 
  | 'TREE_MAP' 
  | 'GRAPH' 
  | 'CHART' 
  | 'TABLE' 
  | 'CODE_VIEW' 
  | 'IMAGE' 
  | 'SEARCH_RESULT';

export interface VisualData {
  type: VisualType;
  title: string;
  data: any;
}
```

**Design Patterns Used:**
- **Enum Pattern** - For finite state machines (PlutoState, CelestialMode)
- **Interface Segregation** - Clean separation of concerns
- **Type Union** - Flexible yet type-safe visual types

---

### 2. Gemini AI Integration

The AI engine uses Google's Gemini API for intelligent conversation processing.

```typescript
// generative.ts
import { GoogleGenerativeAI } from "@google/generative-ai";

const apiKey = import.meta.env.VITE_GEMINI_API_KEY;

console.log("Gemini key:", apiKey);

if (!apiKey) {
  throw new Error(
    "VITE_GEMINI_API_KEY is not defined. Ensure it's set in your Vite env or workflow."
  );
}

export const genAI = new GoogleGenerativeAI(apiKey);

export async function runChatExample() {
  const model = genAI.getGenerativeModel({
    model: "gemini-1.5-flash",
  });

  const chat = model.startChat();

  const result = await chat.sendMessage("Hello Gemini");
  console.log(result.response.text());
  return result;
}

// Usage notes:
// - Import `genAI` where needed and construct models via `genAI.getGenerativeModel(...)`.
// - Read the key from `import.meta.env.VITE_GEMINI_API_KEY` and pass it to the constructor.
// - Do NOT call `new GoogleGenerativeAI()` without the key or read from `process.env` in the browser.
```

**Advanced Implementation Example:**

```typescript
// Enhanced chat with conversation history and system prompts
export async function createEnhancedChat(systemPrompt: string) {
  const model = genAI.getGenerativeModel({
    model: "gemini-1.5-flash",
    systemInstruction: systemPrompt,
    generationConfig: {
      temperature: 0.7,
      topP: 0.95,
      maxOutputTokens: 8192,
    },
  });

  return model.startChat({
    history: [],
  });
}

// Send message with context
export async function sendMessageWithContext(
  chat: any,
  userMessage: string,
  context: {
    celestialMode: CelestialMode;
    previousActions: ActionPlan[];
    userData?: any;
  }
) {
  const enhancedPrompt = `
    Current Context:
    - Celestial Mode: ${context.celestialMode}
    - Previous Actions: ${JSON.stringify(context.previousActions)}
    
    User Message: ${userMessage}
  `;

  const result = await chat.sendMessage(enhancedPrompt);
  return result.response.text();
}
```

---

### 3. Celestial Orb Visualization Component

The visual centerpiece - a dynamic orb that changes appearance based on state and mode.

```typescript
// Orb.tsx
import React from 'react';
import { PlutoState, CelestialMode } from '../types';

interface OrbProps {
  state: PlutoState;
  amplitude: number;
  mode: CelestialMode;
}

const Orb: React.FC<OrbProps> = ({ state, amplitude, mode }) => {
  const getCelestialConfig = () => {
    switch (mode) {
      case CelestialMode.SUN:
        const sunBlink = state === PlutoState.SPEAKING 
          ? Math.sin(Date.now() / 150) * 0.04 + 1 
          : 1;
        return {
          core: 'from-amber-300 via-orange-500 to-red-600',
          glow: 'rgba(251, 191, 36, 0.7)',
          flareSpeed: state === PlutoState.THINKING ? '0.2s' : '0.6s',
          scale: (state === PlutoState.LISTENING ? 1.1 + amplitude * 1.2 : 1.12) * sunBlink,
          coronaOpacity: 0.6,
          labelColor: 'text-orange-400',
          border: 'border-orange-500/20'
        };
      
      case CelestialMode.MOON:
        const moonBlink = state === PlutoState.SPEAKING 
          ? Math.sin(Date.now() / 200) * 0.02 + 1 
          : 1;
        return {
          core: 'from-slate-300 via-slate-400 to-slate-600',
          glow: 'rgba(200, 200, 220, 0.4)',
          flareSpeed: '4s',
          scale: (state === PlutoState.LISTENING ? 1.05 + amplitude * 0.8 : 1.05) * moonBlink,
          coronaOpacity: 0.2,
          labelColor: 'text-slate-400',
          border: 'border-slate-500/20'
        };
      
      case CelestialMode.EARTH:
        const earthBlink = state === PlutoState.SPEAKING 
          ? Math.sin(Date.now() / 180) * 0.03 + 1 
          : 1;
        return {
          core: 'from-blue-600 via-blue-800 to-indigo-900',
          glow: 'rgba(59, 130, 246, 0.5)',
          flareSpeed: '2s',
          scale: (state === PlutoState.LISTENING ? 1.08 + amplitude * 1.0 : 1.08) * earthBlink,
          coronaOpacity: 0.4,
          labelColor: 'text-blue-400',
          border: 'border-blue-500/20'
        };
      
      case CelestialMode.NEUTRAL:
        const neutralBlink = state === PlutoState.SPEAKING 
          ? Math.sin(Date.now() / 120) * 0.02 + 1 
          : 1;
        return {
          core: 'from-slate-900 via-black to-slate-950',
          glow: 'rgba(56, 189, 248, 0.2)',
          flareSpeed: '10s',
          scale: (state === PlutoState.LISTENING ? 1.0 + amplitude * 0.5 : 1.0) * neutralBlink,
          coronaOpacity: 0.1,
          labelColor: 'text-slate-600',
          border: 'border-white/5'
        };
      
      default:
        return {
          core: 'from-slate-800 to-black',
          glow: 'rgba(100, 100, 100, 0.3)',
          flareSpeed: '5s',
          scale: 1.0,
          coronaOpacity: 0.15,
          labelColor: 'text-gray-500',
          border: 'border-gray-500/10'
        };
    }
  };

  const config = getCelestialConfig();

  return (
    <div className="relative flex items-center justify-center w-64 h-64">
      {/* Outer glow */}
      <div
        className="absolute inset-0 rounded-full blur-3xl transition-all duration-300"
        style={{
          background: config.glow,
          transform: `scale(${config.scale})`,
          opacity: config.coronaOpacity,
        }}
      />
      
      {/* Main orb */}
      <div
        className={`relative w-48 h-48 rounded-full bg-gradient-to-br ${config.core} 
          shadow-2xl transition-all duration-500 ease-in-out
          ${config.border} border-2`}
        style={{
          transform: `scale(${config.scale})`,
          animation: `pulse ${config.flareSpeed} ease-in-out infinite`,
        }}
      >
        {/* Inner shimmer */}
        <div className="absolute inset-0 rounded-full bg-gradient-to-tr from-white/20 to-transparent animate-pulse" />
      </div>

      {/* State indicator */}
      <div className={`absolute bottom-0 text-sm font-mono ${config.labelColor}`}> 
        {state}
      </div>
    </div>
  );
};

export default Orb;
```

**Visualization Techniques:**
- **Real-time Animation** - Math.sin() for breathing effects
- **State-driven Styling** - Dynamic colors and scales
- **Performance Optimization** - CSS transforms for smooth animations
- **Gradient Mastery** - Multiple layers for depth

---

## State Management & Types

### State Machine Diagram

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  IDLE   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚
                          â”‚                â”‚
                    User speaks            â”‚
                          â”‚                â”‚
                          â–¼                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                   â”‚  LISTENING   â”‚        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                          â”‚                â”‚
                  Audio captured           â”‚
                          â”‚                â”‚
                          â–¼                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                   â”‚  THINKING    â”‚        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                          â”‚                â”‚
                   AI processes            â”‚
                          â”‚                â”‚
                          â–¼                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                   â”‚  SPEAKING    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                     Has visuals?
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ VISUALIZING  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete State Management Example

```typescript
import { useState, useEffect, useCallback } from 'react';
import { PlutoState, CelestialMode, Message, VisualData } from './types';

export function useAssistantState() {
  const [state, setState] = useState<PlutoState>(PlutoState.IDLE);
  const [mode, setMode] = useState<CelestialMode>(CelestialMode.NEUTRAL);
  const [messages, setMessages] = useState<Message[]>([]);
  const [amplitude, setAmplitude] = useState<number>(0);

  // Audio context for visualizing speech
  const analyzeAudio = useCallback((stream: MediaStream) => {
    const audioContext = new AudioContext();
    const analyzer = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    
    source.connect(analyzer);
    analyzer.fftSize = 256;
    
    const dataArray = new Uint8Array(analyzer.frequencyBinCount);
    
    const updateAmplitude = () => {
      analyzer.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
      setAmplitude(average / 255);
      
      if (state === PlutoState.LISTENING) {
        requestAnimationFrame(updateAmplitude);
      }
    };
    
    updateAmplitude();
  }, [state]);

  // Transition between states
  const transitionState = useCallback((newState: PlutoState) => {
    setState(newState);
    
    // Auto-transition logic
    if (newState === PlutoState.SPEAKING) {
      // After speaking, return to idle
      setTimeout(() => {
        if (state === PlutoState.SPEAKING) {
          setState(PlutoState.IDLE);
        }
      }, 3000);
    }
  }, [state]);

  // Add message with visual data
  const addMessage = useCallback((
    role: 'user' | 'assistant',
    text: string,
    visual?: VisualData
  ) => {
    const message: Message = {
      role,
      text,
      timestamp: Date.now(),
      visual,
    };
    
    setMessages(prev => [...prev, message]);
  }, []);

  return {
    state,
    mode,
    messages,
    amplitude,
    setState: transitionState,
    setMode,
    addMessage,
    analyzeAudio,
  };
}
```

---

## Visual Representation System

### Supported Visual Types

```typescript
// Visual type hierarchy
interface VisualRenderer {
  type: VisualType;
  render: (data: any) => JSX.Element;
}

// Mind Map Visualization
const MindMapVisual: React.FC<{ data: MindMapData }> = ({ data }) => {
  return (
    <div className="mind-map-container">
      <div className="central-node">{data.center}</div>
      {data.branches.map((branch, idx) => (
        <div key={idx} className="branch">
          <div className="branch-label">{branch.label}</div>
          {branch.children.map((child, cidx) => (
            <div key={cidx} className="leaf">{child}</div>
          ))}
        </div>
      ))}
    </div>
  );
};

// Chart Visualization
const ChartVisual: React.FC<{ data: ChartData }> = ({ data }) => {
  return (
    <div className="chart-container">
      <canvas ref={canvasRef} width={600} height={400} />
    </div>
  );
};

// Code View
const CodeVisual: React.FC<{ data: CodeData }> = ({ data }) => {
  return (
    <pre className="code-block">
      <code className={`language-${data.language}`}>    
        {data.code}
      </code>
    </pre>
  );
};
```

### Visual Processing Pipeline

```
AI Response Text
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Parse     â”‚â”€â”€â–º Extract visual markers
â”‚  Response   â”‚    e.g., [CHART], [GRAPH], [TABLE]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classify   â”‚â”€â”€â–º Determine visual type
â”‚   Visual    â”‚    Match pattern to VisualType
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract    â”‚â”€â”€â–º Parse structured data
â”‚    Data     â”‚    JSON, CSV, custom format
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Render    â”‚â”€â”€â–º Select appropriate component
â”‚  Component  â”‚    Pass data to renderer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
   Display
```

---

## Code Implementation

### Complete Main Application Component

```typescript
import React, { useState, useEffect, useRef } from 'react';
import Orb from './components/Orb';
import ChatPanel from './components/ChatPanel';
import { useAssistantState } from './hooks/useAssistantState';
import { genAI, createEnhancedChat } from './generative';
import { PlutoState, CelestialMode } from './types';

const App: React.FC = () => {
  const {
    state,
    mode,
    messages,
    amplitude,
    setState,
    setMode,
    addMessage,
    analyzeAudio,
  } = useAssistantState();

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const synthesisRef = useRef<SpeechSynthesisUtterance | null>(null);
  const chatRef = useRef<any>(null);

  // Initialize Gemini chat
  useEffect(() => {
    const initChat = async () => {
      const systemPrompt = `
        You are Pluto, an intelligent AI assistant with visual capabilities.
        You can respond with text and visual data representations.
        
        When appropriate, include visual markers in your response:
        - [CHART:type:data] for charts
        - [GRAPH:data] for graphs
        - [MINDMAP:data] for mind maps
        - [TABLE:data] for tables
        
        Current mode: ${mode}
      `;
      
      chatRef.current = await createEnhancedChat(systemPrompt);
    };
    
    initChat();
  }, [mode]);

  // Initialize speech recognition
  useEffect(() => {
    if ('webkitSpeechRecognition' in window) {
      const recognition = new webkitSpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = false;
      
      recognition.onstart = () => {
        setState(PlutoState.LISTENING);
      };
      
      recognition.onresult = async (event) => {
        const transcript = event.results[0][0].transcript;
        addMessage('user', transcript);
        
        setState(PlutoState.THINKING);
        
        // Send to AI
        const response = await chatRef.current.sendMessage(transcript);
        const responseText = response.response.text();
        
        // Parse visual data
        const visual = parseVisualData(responseText);
        
        addMessage('assistant', responseText, visual);
        
        // Speak response
        speak(responseText);
      };
      
      recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
        setState(PlutoState.ERROR);
      };
      
      recognitionRef.current = recognition;
    }
  }, []);

  // Text-to-speech
  const speak = (text: string) => {
    const utterance = new SpeechSynthesisUtterance(text);
    
    utterance.onstart = () => {
      setState(PlutoState.SPEAKING);
    };
    
    utterance.onend = () => {
      setState(PlutoState.IDLE);
    };
    
    speechSynthesis.speak(utterance);
    synthesisRef.current = utterance;
  };

  // Start listening
  const startListening = () => {
    if (recognitionRef.current) {
      recognitionRef.current.start();
      
      // Get microphone access for amplitude visualization
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(analyzeAudio)
        .catch(console.error);
    }
  };

  // Parse visual data from AI response
  const parseVisualData = (text: string): VisualData | undefined => {
    const chartMatch = text.match(/\[CHART:(\w+):({.*?})\]/);
    const graphMatch = text.match(/\[GRAPH:({.*?})\]/);
    const mindmapMatch = text.match(/\[MINDMAP:({.*?})\]/);
    const tableMatch = text.match(/\[TABLE:({.*?})\]/);
    
    if (chartMatch) {
      return {
        type: 'CHART',
        title: `${chartMatch[1]} Chart`,
        data: JSON.parse(chartMatch[2]),
      };
    } else if (graphMatch) {
      return {
        type: 'GRAPH',
        title: 'Graph Visualization',
        data: JSON.parse(graphMatch[1]),
      };
    } else if (mindmapMatch) {
      return {
        type: 'MIND_MAP',
        title: 'Mind Map',
        data: JSON.parse(mindmapMatch[1]),
      };
    } else if (tableMatch) {
      return {
        type: 'TABLE',
        title: 'Data Table',
        data: JSON.parse(tableMatch[1]),
      };
    }
    
    return undefined;
  };

  return (
    <div className="app-container min-h-screen bg-gradient-to-br from-slate-900 via-purple-900 to-slate-900">
      {/* Mode selector */}
      <div className="mode-selector absolute top-4 right-4 flex gap-2">
        {Object.values(CelestialMode).map((m) => (
          <button
            key={m}
            onClick={() => setMode(m)}
            className={`px-4 py-2 rounded-lg ${
              mode === m ? 'bg-white text-black' : 'bg-gray-800 text-white'
            }`}
          >
            {m}
          </button>
        ))}
      </div>

      {/* Main content */}
      <div className="flex flex-col items-center justify-center min-h-screen p-8">
        <Orb state={state} amplitude={amplitude} mode={mode} />
        
        <button
          onClick={startListening}
          disabled={state !== PlutoState.IDLE}
          className="mt-8 px-8 py-4 bg-purple-600 text-white rounded-full 
            hover:bg-purple-700 disabled:opacity-50 transition-all"
        >
          {state === PlutoState.IDLE ? 'Start Talking' : state}
        </button>
        
        <ChatPanel messages={messages} />
      </div>
    </div>
  );
};

export default App;
```

---

## Deployment Architecture

### Docker Containerization

```dockerfile
FROM node:20-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install

# Copy source code
COPY . .

# Build application
RUN npm run build

# Expose port
EXPOSE 5000

# Start application
CMD ["npm", "run", "start"]
```

### Replit Configuration

```ini
modules = ["nodejs-20", "web", "postgresql-16"]
run = "npm run dev"
hidden = [".config", ".git", "generated-icon.png", "node_modules", "dist"]

[nix]
channel = "stable-24_05"

[deployment]
deploymentTarget = "autoscale"
build = ["npm", "run", "build"]
run = ["npm", "run", "start"]

[[ports]]
localPort = 5000
externalPort = 80

[workflows]
runButton = "Project"

[[workflows.workflow]]
name = "Project"
mode = "parallel"
author = "agent"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Start application"

[[workflows.workflow]]
name = "Start application"
author = "agent"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "npm run dev"
waitForPort = 5000
```

### Environment Configuration

```bash
# Gemini API
VITE_GEMINI_API_KEY=your_gemini_api_key_here

# Application
VITE_APP_NAME=Pluto Assistant
VITE_APP_VERSION=1.0.0

# Speech Recognition
VITE_SPEECH_LANG=en-US
VITE_SPEECH_CONTINUOUS=false

# Database (Optional)
DATABASE_URL=postgresql://user:password@localhost:5432/assistant_db
```

### Infrastructure Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLOUD PROVIDER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                     LOAD BALANCER                        â”‚  â”‚
â”‚  â”‚                   (Auto-scaling enabled)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                    â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚       â”‚                   â”‚                   â”‚               â”‚
â”‚       â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Server  â”‚         â”‚ Server  â”‚         â”‚ Server  â”‚         â”‚
â”‚  â”‚Instance â”‚         â”‚Instance â”‚         â”‚Instance â”‚         â”‚
â”‚  â”‚   #1    â”‚         â”‚   #2    â”‚         â”‚   #3    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚                   â”‚                   â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                           â”‚                                    â”‚
â”‚                           â–¼                                    â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚               â”‚   PostgreSQL DB       â”‚                        â”‚
â”‚               â”‚   (Persistent State)  â”‚                        â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ External API Calls
                            â”‚
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Google Gemini API     â”‚
              â”‚   (AI Processing)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Best Practices & Patterns

### 1. Error Handling

```typescript
// Comprehensive error boundary
class AssistantErrorBoundary extends React.Component<
  { children: React.ReactNode },
  { hasError: boolean; error?: Error }
> {
  constructor(props: any) {
    super(props);
    this.state = { hasError: false };
  }

  static getDerivedStateFromError(error: Error) {
    return { hasError: true, error };
  }

  componentDidCatch(error: Error, errorInfo: React.ErrorInfo) {
    console.error('Assistant error:', error, errorInfo);
    
    // Log to monitoring service
    logErrorToService(error, errorInfo);
  }

  render() {
    if (this.state.hasError) {
      return (
        <div className="error-screen">
          <h1>Something went wrong</h1>
          <p>{this.state.error?.message}</p>
          <button onClick={() => window.location.reload()}>
            Restart Assistant
          </button>
        </div>
      );
    }

    return this.props.children;
  }
}
```

### 2. Performance Optimization

```typescript
// Memoize expensive computations
const MemoizedOrb = React.memo(Orb, (prevProps, nextProps) => {
  return (
    prevProps.state === nextProps.state &&
    prevProps.mode === nextProps.mode &&
    Math.abs(prevProps.amplitude - nextProps.amplitude) < 0.01
  );
});

// Debounce amplitude updates
const useDebouncedAmplitude = (amplitude: number, delay: number = 16) => {
  const [debouncedValue, setDebouncedValue] = useState(amplitude);

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(amplitude);
    }, delay);

    return () => clearTimeout(handler);
  }, [amplitude, delay]);

  return debouncedValue;
};
```

### 3. Accessibility

```typescript
// ARIA labels and keyboard navigation
<button
  onClick={startListening}
  aria-label="Start voice input"
  aria-pressed={state === PlutoState.LISTENING}
  role="button"
  tabIndex={0}
  onKeyPress={(e) => {
    if (e.key === 'Enter' || e.key === ' ') {
      startListening();
    }
  }}
>
  Start Talking
</button>

// Screen reader announcements
const announceState = (state: PlutoState) => {
  const announcement = document.createElement('div');
  announcement.setAttribute('role', 'status');
  announcement.setAttribute('aria-live', 'polite');
  announcement.textContent = `Assistant is now ${state.toLowerCase()}`;
  document.body.appendChild(announcement);
  
  setTimeout(() => announcement.remove(), 1000);
};
```

### 4. Testing Strategy

```typescript
// Unit test example
describe('Orb Component', () => {
  it('renders with correct celestial mode', () => {
    const { container } = render(
      <Orb 
        state={PlutoState.IDLE} 
        amplitude={0} 
        mode={CelestialMode.SUN} 
      />
    );
    
    expect(container.querySelector('.from-amber-300')).toBeInTheDocument();
  });

  it('scales based on amplitude during listening', () => {
    const { rerender } = render(
      <Orb 
        state={PlutoState.LISTENING} 
        amplitude={0.5} 
        mode={CelestialMode.SUN} 
      />
    );
    
    // Verify scale transform is applied
    const orb = screen.getByRole('img', { hidden: true });
    expect(orb.style.transform).toContain('scale');
  });
});

// Integration test
describe('Voice Interaction Flow', () => {
  it('completes full conversation cycle', async () => {
    const { getByText, findByText } = render(<App />);
    
    // Start listening
    fireEvent.click(getByText('Start Talking'));
    expect(await findByText('LISTENING')).toBeInTheDocument();
    
    // Simulate speech recognition
    simulateSpeechInput('Hello Pluto');
    
    // Verify thinking state
    expect(await findByText('THINKING')).toBeInTheDocument();
    
    // Wait for response
    expect(await findByText('SPEAKING')).toBeInTheDocument();
    
    // Verify message in chat
    expect(await findByText(/Hello/)).toBeInTheDocument();
  });
});
```

---

## Conclusion

This architecture demonstrates a production-ready approach to building intelligent conversational AI assistants with:

âœ… **Strong Type Safety** - TypeScript interfaces for all data structures  
âœ… **Reactive State Management** - Clean state transitions with React hooks  
âœ… **Multi-Modal Interaction** - Voice, text, and visual outputs  
âœ… **Beautiful UI** - Celestial-themed, animated components  
âœ… **Scalable Infrastructure** - Containerized deployment with auto-scaling  
âœ… **Best Practices** - Error handling, performance optimization, accessibility  

### Key Takeaways

1. **Finite State Machines** are ideal for managing conversation flows
2. **Visual feedback** enhances user engagement (Orb animations)
3. **Type safety** prevents runtime errors in complex systems
4. **Modular architecture** allows easy feature additions
5. **Performance matters** - optimize animations and API calls

### Next Steps

- Add multi-language support
- Implement conversation persistence
- Build custom visualization plugins
- Add voice biometrics for personalization
- Create mobile companion app

---

## Resources

- **Source Code**: [GitHub @prathapm14](https://github.com/prathapm14)
- **Gemini API**: [Google AI Studio](https://ai.google.dev/)
- **Web Speech API**: [MDN Documentation](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API)
- **React Documentation**: [react.dev](https://react.dev)

---

*Built with â¤ï¸ by Pratham Reddy | January 2026*